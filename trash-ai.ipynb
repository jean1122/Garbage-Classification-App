{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7I-y0jOJrwJd"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import config\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "import torchvision.models as models\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from math import ceil\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "ray.init(num_cpus=config.NUM_CPUS, num_gpus=config.NUM_GPUS, log_to_driver=False)\n",
    "# %config InlineBackend.figure_formats = ['svg'] # Make plots high resolution\n",
    "plt.rcParams['figure.figsize'] = (12, 12) # Increase figure size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAugmentation(nn.Module):\n",
    "    def __init__(self, apply_color_jitter = False, apply_random_augment = True, *args, **kwarrgs):\n",
    "        super().__init__()\n",
    "        self._apply_color_jitter = apply_color_jitter\n",
    "        self._apply_random_augment = apply_random_augment\n",
    "        \n",
    "        self.rand_augment = transforms.RandAugment(*args, **kwarrgs)\n",
    "        self.jitter = transforms.ColorJitter(0.5, 0.5, 0.5, 0.5)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        if self._apply_color_jitter:\n",
    "            x = self.jitter(x)\n",
    "        if self._apply_random_augment:\n",
    "            x = self.rand_augment(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrashNetDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, transfer_learning=False, augment=True, data_dir=config.ROOT_DIR, batch_size=config.BATCH_SIZE, num_workers=config.NUM_WORKERS, pin_memory=config.PIN_MEMORY):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.augment = augment\n",
    "        self.image_size = 224 if transfer_learning else config.IMAGE_SIZE\n",
    "        # mean and standard deviations computed using seed = 42\n",
    "        self.mean, self.std = (\n",
    "            0.5389, 0.5123, 0.4846), (0.2201, 0.2178, 0.2323)\n",
    "\n",
    "        self.augmentation = DataAugmentation()\n",
    "\n",
    "    # Get indices of the train, validation, and test dataset split equally according to class distribution\n",
    "\n",
    "    def get_indices(self, dataset):\n",
    "        targets = np.asarray(dataset.targets)\n",
    "        train_data_idx, test_idx = train_test_split(\n",
    "            np.arange(len(targets)), test_size=config.TEST_SPLIT, stratify=targets)\n",
    "        train_idx, val_idx = train_test_split(np.arange(len(\n",
    "            train_data_idx)), test_size=config.VAL_SPLIT, stratify=targets[train_data_idx])\n",
    "        train_idx, val_idx = train_data_idx[train_idx], train_data_idx[val_idx]\n",
    "        return train_idx, val_idx, test_idx\n",
    "\n",
    "    # Get samplers from indices\n",
    "    def get_samplers(self, train_idx, val_idx, test_idx):\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        val_sampler = SubsetRandomSampler(val_idx)\n",
    "        test_sampler = SubsetRandomSampler(test_idx)\n",
    "        return train_sampler, val_sampler, test_sampler\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = datasets.ImageFolder(config.ROOT_DIR, transform=transforms.Compose([\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "            transforms.ToTensor()\n",
    "        ]))\n",
    "\n",
    "        train_idx, val_idx, test_idx = self.get_indices(dataset)\n",
    "\n",
    "        # Only calculate the mean and std of train and val dataset. Test idx is hidden.\n",
    "        # self.mean, self.std = self.get_distribution(\n",
    "        #     dataset, np.concatenate([train_idx, val_idx]))\n",
    "        self.train_sampler, self.val_sampler, self.test_sampler = self.get_samplers(\n",
    "            train_idx, val_idx, test_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        transform = [self.augmentation] if self.augment else []\n",
    "        transform = transforms.Compose(transform + [\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(self.mean, self.std)\n",
    "        ])\n",
    "        dataset = datasets.ImageFolder(config.ROOT_DIR, transform=transform)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, sampler=self.train_sampler, num_workers=self.num_workers, pin_memory=self.pin_memory)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(self.mean, self.std)\n",
    "        ])\n",
    "        dataset = datasets.ImageFolder(config.ROOT_DIR, transform=transform)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size,\n",
    "                          sampler=self.val_sampler, num_workers=self.num_workers, pin_memory=self.pin_memory)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(self.mean, self.std)\n",
    "        ])\n",
    "        dataset = datasets.ImageFolder(config.ROOT_DIR, transform=transform)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size,\n",
    "                          sampler=self.test_sampler, num_workers=self.num_workers, pin_memory=self.pin_memory)\n",
    "\n",
    "    def get_distribution(self, data, indices):\n",
    "        def prod(x): return functools.reduce(lambda a, b: a * b, x)\n",
    "        shape = data[0][0].shape\n",
    "        pixels = prod(shape[1:])\n",
    "        print('Calculating Mean...')\n",
    "        mean_unscaled = torch.zeros(shape[0])\n",
    "        for index in indices:\n",
    "            x = data[index][0].flatten(1)\n",
    "            mean_unscaled += torch.sum(x, 1)\n",
    "        mean = mean_unscaled / (len(data) * pixels)\n",
    "        print('Calculating Std...')\n",
    "        std_unscaled = torch.zeros_like(mean)\n",
    "        for index in indices:\n",
    "            x = data[index][0].flatten(1)\n",
    "            std_unscaled += torch.sum(torch.square(x -\n",
    "                                      mean.view(shape[0], 1)), 1)\n",
    "        std = torch.sqrt(std_unscaled / (len(data) * pixels))\n",
    "        mean = mean.item() if prod(mean.shape) == 1 else mean\n",
    "        std = std.item() if prod(std.shape) == 1 else std\n",
    "        print(f'Mean: {mean}, Std: {std}')\n",
    "        return mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrashBaseClass(pl.LightningModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "\n",
    "    def to_one_hot(self, x, ref):\n",
    "        temp = torch.zeros(x.size(0), config.NUM_CLASSES).type_as(ref)\n",
    "        return temp.scatter_(1, x.unsqueeze(1), 1.0)\n",
    "\n",
    "    def log_metrics(self, metric, mode, pred, labels, loss, on_step=True, on_epoch=True):\n",
    "        y_pred = pred.softmax(dim=-1)\n",
    "        metric(y_pred, labels)\n",
    "        metrics = {f'{mode}_accuracy': metric, f'{mode}_loss': loss}\n",
    "        self.log_dict(metrics, on_step=on_step, on_epoch=on_epoch)\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrashBaseline(TrashBaseClass):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(config.IMAGE_SIZE * config.IMAGE_SIZE * config.INPUT_CHANNELS, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, config.NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr = self.hparams.lr, betas=(config.B1, config.B2))\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        one_hot_labels = self.to_one_hot(labels, imgs)\n",
    "        pred = self(imgs).squeeze()\n",
    "        loss = F.cross_entropy(pred, one_hot_labels)\n",
    "        self.log_metrics(self.accuracy, 'train', pred, labels, loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        one_hot_labels = self.to_one_hot(labels, imgs)\n",
    "        pred = self(imgs).squeeze()\n",
    "        loss = F.cross_entropy(pred, one_hot_labels)\n",
    "        metrics = self.log_metrics(self.accuracy, 'val', pred, labels, loss)\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        one_hot_labels = self.to_one_hot(labels, imgs)\n",
    "        pred = self(imgs).squeeze()\n",
    "        loss = F.cross_entropy(pred, one_hot_labels)\n",
    "        metrics = self.log_metrics(self.accuracy, 'test', pred, labels, loss)\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Baseline(lr=0.0005)\n",
    "# dm = TrashNetDataModule()\n",
    "# trainer = pl.Trainer(max_epochs=config.EPOCHS, gpus=1, log_every_n_steps=20)\n",
    "# trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrashResnet50(TrashBaseClass):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        backbone = models.resnet50(pretrained=True)\n",
    "        num_filters = backbone.fc.in_features\n",
    "        layers = list(backbone.children())[:-1]\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "    \n",
    "        num_target_classes = config.NUM_CLASSES\n",
    "        self.classifier = nn.Linear(num_filters, num_target_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.feature_extractor.eval()\n",
    "        with torch.no_grad():\n",
    "            representations = self.feature_extractor(x).flatten(1)\n",
    "        x = self.classifier(representations)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr = self.hparams.lr, betas=(config.B1, config.B2))\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        one_hot_labels = self.to_one_hot(labels, imgs)\n",
    "        pred = self(imgs)\n",
    "        loss = F.cross_entropy(pred, one_hot_labels)\n",
    "        self.log_metrics(self.accuracy, 'train', pred, labels, loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        one_hot_labels = self.to_one_hot(labels, imgs)\n",
    "        pred = self(imgs)\n",
    "        loss = F.cross_entropy(pred, one_hot_labels)\n",
    "        metrics = self.log_metrics(self.accuracy, 'val', pred, labels, loss)\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        one_hot_labels = self.to_one_hot(labels, imgs)\n",
    "        pred = self(imgs)\n",
    "        loss = F.cross_entropy(pred, one_hot_labels)\n",
    "        metrics = self.log_metrics(self.accuracy, 'test', pred, labels, loss)\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_trash_tune(tune_config, checkpoint_dir=config.CHECKPOINT_DIR, transfer_learning=True, num_epochs=config.EPOCHS, num_gpus=config.NUM_GPUS):\n",
    "    lr, batch_size = tune_config['lr'], tune_config['batch_size']\n",
    "    model = TrashResnet50(lr=lr)\n",
    "    dm = TrashNetDataModule(\n",
    "        transfer_learning=transfer_learning, batch_size=batch_size)\n",
    "    checkpoint_callback = pl.callbacks.model_checkpoint.ModelCheckpoint(\n",
    "        dirpath=config.CHECKPOINT_DIR,\n",
    "        filename='{epoch}-{val_loss:.2f}-{val_accuracy:.2f}',\n",
    "        monitor='val_loss'\n",
    "    )\n",
    "    metrics = {'loss': 'val_loss', 'acc': 'val_accuracy'}\n",
    "    tune_callback = TuneReportCallback(metrics, on='validation_end')\n",
    "    trainer = pl.Trainer(max_epochs=num_epochs, gpus=ceil(num_gpus), log_every_n_steps=20,\n",
    "                         enable_progress_bar = False, callbacks=[checkpoint_callback, tune_callback])\n",
    "    trainer.fit(model, dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_config = {\n",
    "    'lr': tune.loguniform(1e-5, 5e-2),\n",
    "    'batch_size': tune.choice([32, 64, 128, 256]),\n",
    "}\n",
    "\n",
    "stopper = tune.stopper.TrialPlateauStopper('loss', 0.001)\n",
    "\n",
    "analysis = tune.run(tune.with_parameters(train_trash_tune), \n",
    "    metric='loss',\n",
    "    mode='min',\n",
    "    local_dir='./results',\n",
    "    resources_per_trial={\n",
    "        'cpu': config.NUM_CPUS,\n",
    "        'gpu': config.NUM_GPUS\n",
    "    },\n",
    "    stop=stopper,\n",
    "    config=tune_config, num_samples=config.NUM_SAMPLES, name='tune_trash_resnet50', max_failures=-1)\n",
    "print(analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Data Exploration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
